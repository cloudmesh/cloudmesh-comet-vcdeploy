# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine={{ slurm_controller }}
ControlAddr={{ slurm_controller }}
AuthType=auth/munge
CacheGroups=0
CryptoType=crypto/munge
MpiDefault=none
ProctrackType=proctrack/pgid
ReturnToService=2
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmd
SwitchType=switch/none
TaskPlugin=task/affinity
TreeWidth=256
#
#
# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0
#
#
# SCHEDULING
FastSchedule=1
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/cons_res
SelectTypeParameters=CR_CPU
#
#
# JOB PRIORITY
#
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
AccountingStoreJobComment=YES
ClusterName={{ slurm_cluster_name }}
JobCompLoc=/var/log/slurm/JobComp.log
JobCompType=jobcomp/filetxt
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmctldDebug=3
SlurmdDebug=3
#
#
# Allocation Setup/Cleanup
#
#
Epilog=/opt/local/slurm/epilog
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#
#

# Generic resources to be managed
# GresTypes=gpu

# COMPUTE NODES
NodeName={{ slurm_compute_nodes }} CPUs={{ slurm_cpus_per_node }} RealMemory={{ slurm_ram_mb_per_node }} Sockets={{ slurm_sockets_per_node }} CoresPerSocket={{ slurm_cores_per_socket }} ThreadsPerCore={{ slurm_threads_per_core }} State=UNKNOWN

PartitionName=general Nodes={{ slurm_compute_nodes }} Default=YES Shared=EXCLUSIVE MaxTime=INFINITE State=UP
